{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3afabbfe",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78591917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import math\n",
    "import pandas as pd\n",
    "from gym import wrappers\n",
    "from collections import deque\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(action='ignore', category=DeprecationWarning, message='`np.bool` is a deprecated alias')\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "from random import sample\n",
    "import torch as T \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributions import Categorical,Normal,MultivariateNormal\n",
    "from multiprocessing.pool import Pool\n",
    "import random\n",
    "from rltorch.network import create_linear_network,BaseNetwork\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "# torch.set_grad_enabled(True)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d02c1c",
   "metadata": {},
   "source": [
    "### Env Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7facef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "num_iotds = 100 #number\n",
    "num_uavs=6\n",
    "X_min = 0 #meters\n",
    "X_max = 200 #meters\n",
    "Y_min = 0 #meters\n",
    "Y_max = 200 #meters\n",
    "O = 9 #number\n",
    "R_obstacles_max = 10 #meters\n",
    "Safe_radius = 2 #meters\n",
    "Cluster_radius = 20 #meters\n",
    "H = 50 #meters\n",
    "E_wake = 0.1 #Watt/s\n",
    "Comm_radius = 20 #meters\n",
    "Wake_radius = 30 #meters\n",
    "B = 500 * 10**6 # hertz , available channel bandwidth\n",
    "beta_o = 31.62 # Watts, path loss at reference distance\n",
    "rho_m = 0.1 # Watts, transmitting power of a CH\n",
    "sigma = 10**-13 # Watts , noise power\n",
    "alpha = 2 # path loss exponent\n",
    "V_max = 10 #m/s\n",
    "V_min = 5 #m/s\n",
    "E_start_UAV = 1000 #Joules\n",
    "E_hover = 100 #Watt\n",
    "U_tip = 200 #m/s\n",
    "v_o = 10 # m/s , mean rotor induced velocity in the hovering state\n",
    "rho = 1.225 # kg/m^3, air density\n",
    "delta = 0.202 # rotor area\n",
    "d_o = 0.1 # fusealage drag ratio , dimensionless\n",
    "s = 0.1 # rotor solidity , dimensionless\n",
    "P_o = 0.7 # blade profile power coefficient , dimensionless\n",
    "P_i = 0.2 # induced power coefficient in the hovering state \n",
    "P_h = 100 #Watt, power required for hovering\n",
    "D_max = 10 # meters\n",
    "D_min = 1 # meters\n",
    "E_IoTD_start = 1000000 #Joules\n",
    "Distance_O_penalty = 100\n",
    "Distance_multiplier = 0.1\n",
    "h_reward = 100000\n",
    "\n",
    "k_E_UAV=0.1\n",
    "k_E_UE=0.1\n",
    "k_AOI=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53601c2",
   "metadata": {},
   "source": [
    "### Policy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 10000\n",
    "max_steps_per_episode=30\n",
    "import copy\n",
    "       \n",
    "action_std = 0.6                    \n",
    "action_std_decay_rate = 0.05       \n",
    "min_action_std = 0.1                \n",
    "\n",
    "  \n",
    "K_epochs = 30        \n",
    "\n",
    "eps_clip = 0.2         \n",
    "gamma = 0.99           \n",
    "max_steps=50\n",
    "update=max_steps*4\n",
    "overallsteps=200000\n",
    "lr_actor = 0.0003      \n",
    "lr_critic = 0.001      \n",
    "string='Multi_Agent_UAV'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72277eae",
   "metadata": {},
   "source": [
    "### utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a842e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import sqrt\n",
    "\n",
    "def calculate_distance(coord1, coord2):\n",
    "    distance = sqrt((coord1[0] - coord2[0])**2 + (coord1[1] - coord2[1])**2)\n",
    "    return distance\n",
    "\n",
    "def calculate_UAV_CH_distance(P_t,P_m):\n",
    "    distance = sqrt((P_t[0] - P_m[0])**2 + (P_t[1] - P_m[1])**2 + H**2)\n",
    "    return distance\n",
    "\n",
    "def calculate_power_gain_UAV_CH(beta_o,P_t,P_m):\n",
    "    distance = calculate_UAV_CH_distance(P_t,P_m)\n",
    "    power_gain = beta_o/(distance**2)\n",
    "    return power_gain\n",
    "\n",
    "def calculate_UAV_CH_transmission_rate(B, P_t, P_m, beta_o, sigma, rho_m):\n",
    "    power_gain = calculate_power_gain_UAV_CH(beta_o, P_t, P_m)\n",
    "    transmission_rate = B * np.log2(1 + (rho_m * power_gain)/(sigma**2))\n",
    "    return transmission_rate\n",
    "\n",
    "def calculate_UAV_CH_Upload_time(B, P_t, P_m, beta_o, sigma, rho_m, I_m):\n",
    "    transmission_rate = calculate_UAV_CH_transmission_rate(B, P_t, P_m, beta_o, B , sigma, rho_m)\n",
    "    upload_time = I_m/transmission_rate\n",
    "    return upload_time\n",
    "\n",
    "def calculate_CH_IoTD_distance(P_m, P_n):\n",
    "    distance = sqrt((P_m[0] - P_n[0])**2 + (P_m[1] - P_n[1])**2)\n",
    "    return distance\n",
    "\n",
    "def calculate_CH_IOTD_transmission_rate(B, P_m, P_n, beta_m,rho_n,alpha , rho_i):\n",
    "    distance = calculate_CH_IoTD_distance(P_m, P_n)\n",
    "    power_gain = beta_m/(distance**alpha)\n",
    "    transmission_rate = B * np.log2(1 + (rho_i * power_gain)/(rho_n))\n",
    "    return transmission_rate\n",
    "\n",
    "def calculate_UAV_Obstacle_distance(P_t,L_o):\n",
    "    distance = sqrt((P_t[0]-L_o[0])**2 + (P_t[1]-L_o[1])**2) - L_o[2]\n",
    "    return distance\n",
    "\n",
    "def calculate_UAV_travel_energy_per_second(P_o,v_t,U_tip,P_i,v_o,d_o,rho,s,delta):\n",
    "    energy_per_second = P_o*(1+3*(v_t**2)/(U_tip**2)) + P_i*v_o/v_t + d_o*rho*s*delta*(v_t**3)/2\n",
    "    return energy_per_second\n",
    "\n",
    "def maximum_minimum_distance_clustering(nodes, cluster_radius):\n",
    "    cluster_heads = []\n",
    "    remaining_nodes = nodes.copy()\n",
    "\n",
    "    first_node=remaining_nodes.pop(0)\n",
    "    \n",
    "    cluster_heads.append((first_node[0],first_node[1]))\n",
    "\n",
    "    while remaining_nodes:\n",
    "        max_min_distance = 0\n",
    "        max_min_distance_node = None\n",
    "        for node in remaining_nodes:\n",
    "            min_distance = min(calculate_distance(node, ch) for ch in cluster_heads)\n",
    "            if min_distance > max_min_distance and min_distance > cluster_radius:\n",
    "                max_min_distance = min_distance\n",
    "                max_min_distance_node = node\n",
    "        if max_min_distance_node is None:\n",
    "            break\n",
    "        cluster_heads.append((max_min_distance_node[0],max_min_distance_node[1]))\n",
    "        remaining_nodes.remove(max_min_distance_node)\n",
    "\n",
    "    # print(cluster_heads)\n",
    "    clusters = {ch: [] for ch in cluster_heads}\n",
    "    for node in remaining_nodes:\n",
    "        nearest_ch = min(cluster_heads, key=lambda ch: calculate_distance(node, ch))\n",
    "        clusters[nearest_ch].append(node)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def generate_IoTD_coordinates_outside_obstacles(num_coordinates, Obstacle_coordinates):\n",
    "    coordinates = []\n",
    "    xmin = X_min\n",
    "    xmax = X_max\n",
    "    ymin = Y_min\n",
    "    ymax = Y_max\n",
    "    for i in range(num_coordinates):\n",
    "        x = random.uniform(xmin, xmax)\n",
    "        y = random.uniform(ymin, ymax)\n",
    "        new_coord = (x, y)\n",
    "        if not any(calculate_distance(new_coord, obstacle) <= obstacle[2] + Safe_radius for obstacle in Obstacle_coordinates):\n",
    "            coordinates.append(new_coord)\n",
    "    \n",
    "    return coordinates\n",
    "\n",
    "    \n",
    "\n",
    "def generate_Obstacle_coordinates(num_coordinates):\n",
    "    coordinates = []\n",
    "    xmin = X_min\n",
    "    xmax = X_max\n",
    "    ymin = Y_min\n",
    "    ymax = Y_max\n",
    "    for i in range(num_coordinates):\n",
    "        x = random.uniform(xmin, xmax)\n",
    "        y = random.uniform(ymin, ymax)\n",
    "        r = random.uniform(0, R_obstacles_max)\n",
    "        new_coord = (x, y, r)\n",
    "        coordinates.append(new_coord)\n",
    "    \n",
    "    return coordinates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd41123c",
   "metadata": {},
   "source": [
    "### world generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c2ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from math import sqrt\n",
    "\n",
    "coordinates_file_name=\"coordinates.csv\"\n",
    "\n",
    "\n",
    "def calculate_distance(coord1, coord2):\n",
    "    distance = sqrt((coord1[0] - coord2[0])**2 + (coord1[1] - coord2[1])**2)\n",
    "#     print(distance)\n",
    "    return distance\n",
    "\n",
    "def save_to_csv(filename, coordinates):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['X', 'Y'])\n",
    "        csv_writer.writerows(coordinates)\n",
    "\n",
    "X=[(random.randint(X_min,X_max),random.randint(Y_min,Y_max)) for _ in range(num_iotds)]\n",
    "print(X)\n",
    "save_to_csv(coordinates_file_name, X)\n",
    "\n",
    "print(f\"{num_iotds} coordinates generated and saved to {coordinates_file_name}.\")\n",
    "def distance(u,v,w,x,y,z):\n",
    "    distance = ((u-x)**2 + (v-y)**2 + (w-z)**2)**0.5\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b173a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "O_obstacles = [\n",
    "    [50,50,10],\n",
    "    # [100,50,10],\n",
    "    [150,50,10],\n",
    "    # [50,100,10],\n",
    "    # [150,100,10],\n",
    "    [50,150,10],\n",
    "    # [100,150,10],\n",
    "    [150,150,10]\n",
    "]\n",
    "import json\n",
    "def save_coordinates(coords):\n",
    "    with open(\"results/coordinates.json\",\"w\") as f:\n",
    "        f.write(json.dumps(coords))\n",
    "\n",
    "def load_coordinates():\n",
    "    with open(\"results/coordinates.json\",\"r\") as f:\n",
    "        return json.loads(f.read())\n",
    "\n",
    "\n",
    "# N = generate_IoTD_coordinates_outside_obstacles(num_iotds, O_obstacles)\n",
    "# save_coordinates(N)\n",
    "\n",
    "\n",
    "N=load_coordinates()\n",
    "\n",
    "\n",
    "# N=[\n",
    "#     [25,75],\n",
    "#     [25,125],\n",
    "#     [75,75],\n",
    "#     [75,125],\n",
    "#     [125,75],\n",
    "#     [125,125],\n",
    "#     [175,75],\n",
    "#     [175,125],\n",
    "#     [75,25],\n",
    "#     [125,25],\n",
    "#     [75,175],\n",
    "#     [125,175],\n",
    "    \n",
    "# ] \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot IoTDs\n",
    "# for coord in N:\n",
    "#     plt.scatter(coord[0], coord[1], color='blue', label='IoTD')\n",
    "    \n",
    "# Plot Obstacles\n",
    "# for obstacle in O_obstacles:\n",
    "#     circle = plt.Circle((obstacle[0], obstacle[1]), obstacle[2], color='red', fill=False)\n",
    "#     plt.gca().add_patch(circle)\n",
    "\n",
    "# Set plot limits\n",
    "# plt.xlim(X_min - 10, X_max + 10)\n",
    "# plt.ylim(Y_min - 10, Y_max + 10)\n",
    "\n",
    "# Add legend\n",
    "# plt.legend()\n",
    "# plt.gca().set_aspect('equal')\n",
    "\n",
    "# Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bcf932",
   "metadata": {},
   "source": [
    "### clustering and location plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c197b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "X=N\n",
    "start_time=time.time()\n",
    "Clusters = maximum_minimum_distance_clustering(X, Cluster_radius)\n",
    "print(f\"clustering time : {time.time()-start_time}\")\n",
    "X=list(Clusters.keys())\n",
    "print(X)\n",
    "M = []\n",
    "\n",
    "for cluster_head, cluster_nodes in Clusters.items():\n",
    "    M.append(cluster_head)\n",
    "\n",
    "M=X.copy()\n",
    "\n",
    "x,y=[node[0] for node in Clusters.keys()],[node[1] for node in Clusters.keys()]\n",
    "for i in Clusters.keys():\n",
    "    color=(random.choice(range(256))/255,\n",
    "           random.choice(range(256))/255,\n",
    "           random.choice(range(256))/255)\n",
    "    plt.scatter(i[0],i[1],color=color,marker='*')\n",
    "    for j in Clusters[i]:\n",
    "        plt.scatter(j[0],j[1],color=color)\n",
    "for obstacle in O_obstacles:\n",
    "    circle = plt.Circle((obstacle[0], obstacle[1]), obstacle[2], color='red', fill=False)\n",
    "    plt.gca().add_patch(circle)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()\n",
    "print(len(x))\n",
    "Data = []\n",
    "\n",
    "for cluster_head , cluster_nodes in Clusters.items():\n",
    "    Data_cluster = 0\n",
    "    for node in range(len(cluster_nodes)+1):\n",
    "        Data_cluster += random.randint(1, 10)\n",
    "    Data.append(Data_cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108e6b20",
   "metadata": {},
   "source": [
    "### Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Multi_Agent_UAV(gym.Env):\n",
    "    def __init__(self, initial_UAV_state = None):\n",
    "        super(Multi_Agent_UAV, self).__init__()\n",
    "\n",
    "        self.agent = num_uavs\n",
    "        self.pos_list = []\n",
    "        self.color = ['red', 'green', 'blue', 'cyan', 'magenta']\n",
    "        self.initial_UAV_state = np.array([X_max/2,Y_max/2])\n",
    "        self.UE_count = len(X)\n",
    "        self.Z = 20\n",
    "        self.max_angle = 2 * math.pi\n",
    "        self.phi_n = np.radians(42.44)  # in degrees\n",
    "        self.C_max_t = (self.Z / np.tan(self.phi_n))\n",
    "        self.boundary_x = self.boundary_y = X_max\n",
    "        self.max_episode_steps = max_steps_per_episode # Maximum number of steps in a single episode, after which environment returns done = True\n",
    "        self._max_episode_steps = max_steps_per_episode\n",
    "        self.current_episode_timestep = 0\n",
    "        self.done = False\n",
    "        self.visited = np.array([0 for i in range(self.UE_count)])\n",
    "        self.state_space_lb = np.array([0, 0], dtype = np.float32)\n",
    "        self.state_space_ub = np.array([self.boundary_x, self.boundary_y], dtype = np.float32)\n",
    "        self.obstacle_count=len(O_obstacles)\n",
    "        self.data=Data\n",
    "        \n",
    "        # plotting parameters\n",
    "        self.plotting_boundary_buffer = 5.\n",
    "        self.UAV_coverage_circle_color = 'lavender'\n",
    "        self.UAV_path_color = 'green'\n",
    "        obh = []\n",
    "        obl = []\n",
    "        self.weight = []\n",
    "        self.visited = np.array([0.0 for i in range(self.UE_count)])\n",
    "        \n",
    "        self.energy_hover=None\n",
    "        self.UE_energy_used=None\n",
    "        \n",
    "        self.energy_array=[]\n",
    "        self.AOI_array=[]\n",
    "        \n",
    "        \n",
    "        # coverage status of IOTDS [self.UE_count], index of uav[1], current state of UAV[2], previous state of UAV[2],uav energy[1], AOI [self.UE_count]\n",
    "        for i in range(self.UE_count+6+self.UE_count):\n",
    "            obh.append(float('inf')) \n",
    "            obl.append(float('-inf'))\n",
    "        \n",
    "        # distance from obstacles\n",
    "        for i in range(self.obstacle_count):\n",
    "            obh.append(float('inf'))\n",
    "            obl.append(float('-inf'))\n",
    "        \n",
    "        # UE data\n",
    "        for i in range(self.UE_count):\n",
    "            obh.append(self.data[i])\n",
    "            obl.append(0)\n",
    "        \n",
    "        # UE energy\n",
    "        for i in range(self.UE_count):\n",
    "            obh.append(float('inf'))\n",
    "            obl.append(float('-inf'))\n",
    "        \n",
    "        \n",
    "        high = np.array(obh, dtype=np.float32)\n",
    "        low = np.array(obl, dtype=np.float32)\n",
    "        high1 = np.array([len(X), 180.0, 6.0], dtype=np.float32)\n",
    "        low1 = np.array([0, -180.0, 5.0], dtype=np.float32)\n",
    "        \n",
    "        self.observation_space = np.array([spaces.Box(low, high) for i in range(self.agent)])\n",
    "        #self.action_space = spaces.Box(low=low1, high=high1, dtype=np.float32)\n",
    "        self.action_space = spaces.Box(-1, 1, (2,), dtype=np.float32)\n",
    "\n",
    "        self.place_UEs(position=\"random\")\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        # RANDOMLY SELECTS UAV STARTING POSITION\n",
    "        self.current_state = self.select_random_state()\n",
    "        ground_UAV_state = self.current_state[:self.agent]\n",
    "        self.visited = np.array([0 for i in range(self.UE_count)])\n",
    "        self.timesteps_in_episode = 0\n",
    "        self.done = False\n",
    "        self.c_state = []\n",
    "        self.AOI=[0 for _ in range(self.UE_count)]\n",
    "        self.UAV_energy=[E_start_UAV for _ in range(self.agent)]\n",
    "        self.UE_energy=[0 for _ in range(self.UE_count)]\n",
    "        self.data=Data\n",
    "        \n",
    "        \n",
    "        for j in range(self.agent):\n",
    "            \n",
    "            distance_from_obstacles = [calculate_UAV_Obstacle_distance(self.current_state[j], obstacle) for obstacle in O_obstacles]            \n",
    "            \n",
    "            obs = []\n",
    "            obs.append(j)\n",
    "            obs.append(self.current_state[j][0].copy()/1000)\n",
    "            obs.append(self.current_state[j][1].copy()/1000)\n",
    "            obs.append(self.current_state[j][0].copy()/1000)\n",
    "            obs.append(self.current_state[j][1].copy()/1000)\n",
    "            for i in range(len(X)):\n",
    "                obs.append(-(math.dist(self.UE_positions[i], ground_UAV_state[j]))/1000)\n",
    "            obs.append(self.UAV_energy[j]/2000)\n",
    "            obs+=[i/200 for i in self.AOI]\n",
    "            \n",
    "            obs+=[i/1000 for i in distance_from_obstacles]\n",
    "            \n",
    "            obs+=[i/400 for i in self.data]\n",
    "            obs+=[i/2000 for i in self.UE_energy]\n",
    "            \n",
    "            self.c_state.append(obs)\n",
    "        self.c_state = np.array(self.c_state)\n",
    "        return self.c_state\n",
    "    \n",
    "    \n",
    "    def get_count_of_UEs_covered(self):\n",
    "        profit = 0\n",
    "        ground_UAV_state = self.current_state[:self.agent]\n",
    "        for j in range(self.agent):\n",
    "            for i in range(self.UE_count):\n",
    "                if(math.dist(self.UE_positions[i], ground_UAV_state[j])<=self.C_max_t):\n",
    "                    if(self.visited[i]==0):\n",
    "                        profit = profit + 500\n",
    "                        self.visited[i] =1\n",
    "                        self.AOI[i]=0\n",
    "                        \n",
    "                        # energy used calculation\n",
    "                        R = calculate_UAV_CH_transmission_rate(B, self.current_state[j], M[i], beta_o, sigma, rho_m)\n",
    "                        time_hover = self.data[i]/R\n",
    "                        self.UE_energy_used[i]=rho_m * self.data[i]/R\n",
    "                        \n",
    "                        # updating the energy of the UAV\n",
    "                        energy_hover = time_hover * P_h\n",
    "                        self.energy_hover[j]+=energy_hover\n",
    "                        self.UE_energy[i] += rho_m * self.data[i]/R\n",
    "                        self.data[i] = 0\n",
    "                        \n",
    "                    else:\n",
    "                        profit = profit-100\n",
    "                        self.AOI[i]+=1\n",
    "                else:\n",
    "                    if(self.visited[i]==0):\n",
    "                        profit = profit -1\n",
    "                        self.AOI[i]+=1\n",
    "#         horizontal_dist_UE_UAV = np.linalg.norm(self.UE_positions - ground_UAV_state, axis = 1)\n",
    "        # print(\"horizontal distance b/w UEs and UAV: \", horizontal_dist_UE_UAV)\n",
    "#         rho_array = (horizontal_dist_UE_UAV <= (self.C_max_t)) * 1  # binary association vector\n",
    "#         M_t = rho_array.sum()\n",
    "        return profit\n",
    "\n",
    "\n",
    "    def render(self, **kwargs):\n",
    "        '''\n",
    "        Function to visualize UAV position\n",
    "        (this function has different configurations for different use cases)\n",
    "        '''\n",
    "        if 'UAV_positions_list' in kwargs:\n",
    "            self.render_UAV_movement_through_episode(UAV_positions_list = kwargs['UAV_positions_list'])\n",
    "\n",
    "        if 'fig' not in kwargs:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "            ax = np.array([ax])\n",
    "            kwargs['fig'] = fig\n",
    "            kwargs['ax'] = ax\n",
    "            kwargs['i'] = 0\n",
    "        \n",
    "        if 'reward' not in kwargs:\n",
    "            kwargs['reward'] = 'Not known'\n",
    "        self.render_position_plot(**kwargs)\n",
    "\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "        '''\n",
    "        Takes action moving environment from current_state to next_state\n",
    "        Arguments: `action` to be taken\n",
    "        Returns: new_state, reward, done, info(=None)\n",
    "        '''\n",
    "        # action = action.numpy().flatten()\n",
    "#         err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "#         assert self.action_space.contains(action), err_msg\n",
    "#         assert self.current_state is not None, \"Call reset before using step method.\"\n",
    "        self.timesteps_in_episode += 1\n",
    "        prev_UAV_state = self.current_state.copy()\n",
    "        _, inside_bounds = self.move(action)\n",
    "        \n",
    "        # print(f\"prev state : {prev_UAV_state} new state : {self.current_state}\")\n",
    "        energy_used=[0 for _ in range(self.agent)]\n",
    "        \n",
    "        self.energy_hover=[0 for i in range(self.agent)]\n",
    "        self.UE_energy_used=[0 for i in range(self.UE_count)]\n",
    "        \n",
    "        \n",
    "        distance_penalty=0\n",
    "        for i in range(self.agent):\n",
    "            velocity = (action[i][0]+1.5)*5 + 2.5  # velocity of the UAV\n",
    "            velocity = v_o\n",
    "            step_size = (action[i][0]+1.2)*10    # step size of the UAV\n",
    "            angle = (action[i][1])*np.pi  # angle of the UAV\n",
    "            \n",
    "            time_flew = calculate_distance(prev_UAV_state[i], self.current_state[i])/velocity\n",
    "            # Calculating the energy used by the UAV in the current step\n",
    "            energy_used[i] = time_flew * calculate_UAV_travel_energy_per_second(P_o, velocity, U_tip, P_i, v_o, d_o, rho, s, delta)\n",
    "            \n",
    "            self.UAV_energy[i]-=energy_used[i]\n",
    "            \n",
    "            \n",
    "            distance_from_obstacles = [calculate_UAV_Obstacle_distance(self.current_state[i], obstacle) for obstacle in O_obstacles]\n",
    "\n",
    "            #calculating the distance penalty of the UAV from the obstacles:\n",
    "            for i in range(self.obstacle_count):\n",
    "                if distance_from_obstacles[i] <= Safe_radius:\n",
    "                    distance_penalty += Distance_O_penalty\n",
    "                # else :\n",
    "                #     distance_penalty += Distance_multiplier * self.distance_from_obstacles[i]\n",
    "        \n",
    "        \n",
    "        # print(type(self.current_state), type(prev_UAV_state))\n",
    "#         if not inside_bounds:\n",
    "#             MENTIONED OUTPUT IS OUT OF BOUNDS\n",
    "#             self.is_done(end = True)\n",
    "#             reward = -10000\n",
    "#             return self.current_state, reward, self.done, None\n",
    "\n",
    "#         ground_UAV_state = self.current_state[:2]\n",
    "#         horizontal_dist_UE_UAV = np.linalg.norm(self.UE_positions - ground_UAV_state, axis = 1)\n",
    "#         rho_array = (horizontal_dist_UE_UAV <= (self.C_max_t)) * 1  # binary association vector\n",
    "#         # self.current_state = np.concatenate((self.current_state[:2], rho_array))\n",
    "#         M_t = rho_array.sum()  # no. of UEs served by the agent\n",
    "\n",
    "        reward = self.get_count_of_UEs_covered()\n",
    "        \n",
    "        for i in range(self.agent):\n",
    "            energy_used[i]+=self.energy_hover[i]\n",
    "        \n",
    "        for i in range(self.agent):\n",
    "            if self.UAV_energy[i]<0:\n",
    "                reward-=10000\n",
    "            else:\n",
    "                reward-=k_E_UAV*energy_used[i]\n",
    "        \n",
    "        avg_AOI=sum(self.AOI)/len(self.AOI)\n",
    "        reward-=k_AOI*avg_AOI\n",
    "        \n",
    "        \n",
    "        for i in range(self.UE_count):\n",
    "            reward-=k_E_UE*self.UE_energy_used[i]\n",
    "        \n",
    "        reward-=distance_penalty\n",
    "        \n",
    "        \n",
    "        # for i in range(self.UE_count):\n",
    "        #     reward-=k_AOI*self.AOI[i]\n",
    "        \n",
    "#         if(sum(self.visited)>15):\n",
    "#             reward = 100 * sum(self.visited) \n",
    "        self.is_done(sum(self.visited))\n",
    "        if self.is_done:\n",
    "            self.energy_array.append(self.UAV_energy)\n",
    "            self.AOI_array.append(avg_AOI)\n",
    "        self.c_state=[]\n",
    "        ground_UAV_state = self.current_state[:self.agent]\n",
    "        for j in range(self.agent):\n",
    "            obs = []\n",
    "            obs.append(j)\n",
    "            obs.append(self.current_state[j][0].copy()/1000)\n",
    "            obs.append(self.current_state[j][1].copy()/1000)\n",
    "            obs.append(prev_UAV_state[j][0].copy()/1000)\n",
    "            obs.append(prev_UAV_state[j][1].copy()/1000)\n",
    "            for i in range(len(X)):\n",
    "                if(self.visited[i] == 0):\n",
    "                    obs.append(-(math.dist(self.UE_positions[i], ground_UAV_state[j]))/10000)\n",
    "                else:\n",
    "                    obs.append(1)\n",
    "            obs.append(self.UAV_energy[j]/2000)\n",
    "            obs+=[i/200 for i in self.AOI]\n",
    "            obs+=[i/1000 for i in distance_from_obstacles]\n",
    "            \n",
    "            obs+=[i/400 for i in self.data]\n",
    "            obs+=[i/2000 for i in self.UE_energy]\n",
    "            \n",
    "            self.c_state.append(obs)\n",
    "        self.c_state = np.array(self.c_state)\n",
    "        for i in range(self.agent):\n",
    "            for j in range(self.agent):\n",
    "                if(i!=j):\n",
    "                    dis = distance(self.current_state[i][0],self.current_state[i][1], self.current_state[j][0], self.current_state[j][1],0,0)\n",
    "                    if(dis<10):\n",
    "                        reward = reward -10000\n",
    "        return self.c_state, reward, self.done, None\n",
    "    \n",
    "\n",
    "    def move(self, action: np.ndarray):\n",
    "        '''\n",
    "        Helper function to step() function.\n",
    "        Clips the passed action to fit within action space bounds.\n",
    "        Calculates new state after performing the passed action, and updates UAV position accordingly. \n",
    "        '''\n",
    "#         evaluates new state reached upon performing the move and saves it in self.current_state\n",
    "        # ACTION = [dx, dy]\n",
    "#         angle1= math.degrees(math.atan(action))\n",
    "#         print(angle1)\n",
    "        for i in range(self.agent):\n",
    "            self.action_step_size = (action[i][0]+1.5)*10\n",
    "            angle1 = action[i][1] * 180\n",
    "            angle = angle1*math.pi/180\n",
    "            x_next = self.current_state[i][0] + (math.cos(angle) * self.action_step_size)\n",
    "            y_next = self.current_state[i][1] + (math.sin(angle) * self.action_step_size)\n",
    "            # updating horizontal_direction_angle if the new move is out of boundary\n",
    "            if((x_next < 0) or (x_next > self.boundary_x) or (y_next < 0) or (y_next > self.boundary_y)):\n",
    "            # UAV REMAINS IN ITS CURRENT POSITION\n",
    "                return action, False\n",
    "\n",
    "            self.current_state[i][:2] = np.array([x_next, y_next])\n",
    "        return action, True\n",
    "\n",
    "\n",
    "    def is_done(self, M_t=0, end=False):\n",
    "        '''\n",
    "        Helper function to check if episode needs to be terminated\n",
    "        '''\n",
    "        if end == True:\n",
    "            self.done = True\n",
    "        elif(sum(self.visited)==self.UE_count):\n",
    "            print(sum(self.visited))\n",
    "            self.done = True\n",
    "        elif(self.timesteps_in_episode >= self.max_episode_steps):\n",
    "            self.done = True\n",
    "        return\n",
    "\n",
    "    \n",
    "    def select_random_state(self):\n",
    "        '''\n",
    "        Selects (and returns) random initial state (within bounds) for the UAV\n",
    "        '''\n",
    "        UAV_pos_list = []\n",
    "        for _ in range(self.agent):\n",
    "            new_x = X_max/2\n",
    "            new_y = Y_max/2\n",
    "\n",
    "            UAV_pos_list.append(np.array([new_x, new_y]))\n",
    "\n",
    "        return np.array(UAV_pos_list)\n",
    "\n",
    "    def place_UEs_randomly(self, random_UE_count = None):\n",
    "        '''\n",
    "        Helper function to place_UEs\n",
    "        places `random_UE_count` UEs randomly onto the rectangular region\n",
    "        if kwargs has the key `exclude_center`, then the circular region spanned by `exclude_center` \\\n",
    "        and `exclude_radius` is excluded\n",
    "        '''\n",
    "\n",
    "        if random_UE_count is None:\n",
    "          random_UE_count = self.UE_count\n",
    "\n",
    "        # places UE_count UEs on grid randomly\n",
    "        if type(self.UE_positions) == np.ndarray:\n",
    "            self.UE_positions = self.UE_positions.tolist()\n",
    "\n",
    "        randomly_placed_count = 0\n",
    "        while(randomly_placed_count < random_UE_count):\n",
    "            x = X[randomly_placed_count][0]\n",
    "            y = X[randomly_placed_count][1]\n",
    "            coords = np.array([x, y])\n",
    "            self.UE_positions.append(coords)\n",
    "            randomly_placed_count += 1\n",
    "        \n",
    "\n",
    "        self.UE_positions = np.array(self.UE_positions)\n",
    "\n",
    "\n",
    "    def place_UEs(self, position=\"random\"):\n",
    "        '''\n",
    "        Function to place UEs onto the rectangular region\n",
    "        '''\n",
    "        self.UE_positions = np.zeros((0, 2))\n",
    "        self.place_UEs_randomly(self.UE_count)\n",
    "\n",
    "        # saves in self.UE_positions\n",
    "        self.UE_positions = np.array(self.UE_positions)\n",
    "        return\n",
    "    def render_UAV_movement_through_episode(self, **kwargs):\n",
    "        '''\n",
    "        Helper function to visualize UAV movement through an episode\n",
    "        Arguments: A list specifying UAV positions throughout the episode\n",
    "        '''\n",
    "\n",
    "        if 'reward' not in kwargs:\n",
    "            kwargs['reward'] = 'Not known'\n",
    "        \n",
    "        buffer = 5.\n",
    "        fig, ax = plt.subplots(1, figsize=(10,10))\n",
    "        ax.set_xlim(-buffer, self.boundary_x+buffer)\n",
    "        ax.set_ylim(-buffer, self.boundary_y+buffer)\n",
    "        ax.grid()\n",
    "        \n",
    "        for j in range(len(X)):\n",
    "            UE_x1 = X[j][0]\n",
    "            UE_y1 = X[j][1]\n",
    "            if(self.visited[j] == 0):\n",
    "                ax.plot(UE_x1, UE_y1, color=self.color[0], marker='o', markersize=6, linestyle = '') \n",
    "            else:\n",
    "                ax.plot(UE_x1, UE_y1, color=self.color[1], marker='o', markersize=6, linestyle = '')\n",
    "        \n",
    "        for i in range(self.agent):\n",
    "            ax.plot(kwargs['UAV_positions_list'][:, i, 0] , kwargs['UAV_positions_list'][:, i, 1], color=self.UAV_path_color, markersize=6, linestyle = '-') #, label = \"UAV Path Color\")\n",
    "            ax.plot(kwargs['UAV_positions_list'][-1, i, 0], kwargs['UAV_positions_list'][-1, i, 1], color='blue', marker='x', markersize=9, linestyle = '') #, label = \"UAV ending position\")\n",
    "            \n",
    "            UAV_coverage_area = plt.Circle((kwargs['UAV_positions_list'][-1, i, 0], kwargs['UAV_positions_list'][-1, i, 1]), self.C_max_t, color = self.UAV_coverage_circle_color)\n",
    "            ax.add_artist(UAV_coverage_area)\n",
    "        \n",
    "        for obstacle in O_obstacles:\n",
    "            circle = plt.Circle((obstacle[0], obstacle[1]), obstacle[2], color='red', fill=False)\n",
    "            ax.add_patch(circle)\n",
    "\n",
    "        ax.set_aspect(1)\n",
    "        # naming the x axis\n",
    "        ax.set_xlabel('X pos (m)')\n",
    "        # naming the y axis\n",
    "        ax.set_ylabel('Y pos (m)')\n",
    "        ax.set_title('UAV movement through episode')\n",
    "        # giving a title to my graph\n",
    "        # plt.title('Visually Appealing!')\n",
    "\n",
    "        # show a legend on the plot\n",
    "        ax.legend()\n",
    "        # clear_output()\n",
    "        # display(fig)\n",
    "        # plt.show()\n",
    "        return\n",
    "    \n",
    "    def render_position_plot(self, **kwargs):\n",
    "        '''\n",
    "        Helper function to render(), plots the current position plot on given axes. \n",
    "        Plotting position plot of UAV's current position and coverage\n",
    "        '''\n",
    "        ax = kwargs['ax']\n",
    "        plot_number = kwargs['i']\n",
    "        idx = 0\n",
    "        reward = kwargs['reward']\n",
    "\n",
    "        buffer = 5.\n",
    "        ax[plot_number].set_xlim(-buffer, self.boundary_x+buffer)\n",
    "        ax[plot_number].set_ylim(-buffer, self.boundary_y+buffer)\n",
    "        ax[plot_number].grid()\n",
    "        for j in range(len(X)):\n",
    "            UE_x1 = X[j][0]\n",
    "            UE_y1 = X[j][1]\n",
    "            if(self.visited[j] ==0):\n",
    "                ax[plot_number].plot(UE_x1, UE_y1, color=self.color[0], marker='o', markersize=6, linestyle = '')\n",
    "            else:\n",
    "                ax[plot_number].plot(UE_x1, UE_y1, color=self.color[1], marker='o', markersize=6, linestyle = '')\n",
    "         \n",
    "        \n",
    "        for i in range(self.agent):\n",
    "            ax[plot_number].plot(self.current_state[i][0], self.current_state[i][1], color='blue', marker='x', markersize=9, linestyle = '', label = \"UAV\")\n",
    "            UAV_coverage_area = plt.Circle((self.current_state[i][0] , self.current_state[i][1] ), self.C_max_t, color = self.UAV_coverage_circle_color)\n",
    "            ax[plot_number].add_artist(UAV_coverage_area)\n",
    "      \n",
    "\n",
    "        if 'learnt_policy_visualization' in kwargs:\n",
    "            position_action_list = kwargs['position_action_list']\n",
    "            for position_action in position_action_list:\n",
    "                position = position_action[0]\n",
    "                action = position_action[1]\n",
    "                action = np.clip(action, self.action_space_lb, self.action_space_ub)\n",
    "                action = (((self.action_space_coversion_ub - self.action_space_coversion_lb) * (action + 1)) / 2) + self.action_space_coversion_lb\n",
    "                print(\"scaled action: \", action)\n",
    "                # print(\"action after scaling: \", action)\n",
    "                new_state = self.move(action, get_new_state=True, provided_center=position)\n",
    "                print(\"previous state: \", position, \", new state: \", new_state)\n",
    "                # print(\"new_state: \", new_state)\n",
    "                ax[plot_number].plot(position[0], position[1], color='blue', marker='o', markersize=6, linestyle = '')\n",
    "                ax[plot_number].plot(new_state[0], new_state[1], color='orange', marker='o', markersize=6, linestyle = '')\n",
    "                ax[plot_number].arrow(position[0], position[1], new_state[0] - position[0], new_state[1] - position[1], head_width=0.2, head_length=0.1)\n",
    "\n",
    "\n",
    "        ax[plot_number].set_aspect(1)\n",
    "        # naming the x axis\n",
    "        ax[plot_number].set_xlabel('X pos (m)')\n",
    "        # naming the y axis\n",
    "        ax[plot_number].set_ylabel('Y pos (m)')\n",
    "        ax[plot_number].set_title('reward: ' + str(reward))\n",
    "        # giving a title to my graph\n",
    "        # plt.title('Visually Appealing!')\n",
    "\n",
    "        # show a legend on the plot\n",
    "        ax[plot_number].legend()\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Multi_Agent_UAV()\n",
    "state = env.reset()\n",
    "UAV_positions_list = []\n",
    "ab=[]\n",
    "for i in range(env.agent):\n",
    "    k = []\n",
    "    k.append(env.current_state[i][0])\n",
    "    k.append(env.current_state[i][1])\n",
    "    ab.append(k)\n",
    "UAV_positions_list.append(ab)\n",
    "a = [env.action_space.sample() for i in range(env.agent)]\n",
    "# print(a)\n",
    "next_state, reward, done, _ = env.step(a)\n",
    "# print(next_state, reward)\n",
    "ab=[]\n",
    "for i in range(env.agent):\n",
    "    k = []\n",
    "    k.append(env.current_state[i][0])\n",
    "    k.append(env.current_state[i][1])\n",
    "    ab.append(k)\n",
    "UAV_positions_list.append(ab)\n",
    "b = [env.action_space.sample() for i in range(env.agent)]\n",
    "# print(b)\n",
    "next_state, reward, done, _ = env.step(b)\n",
    "# print(next_state, reward)\n",
    "ab=[]\n",
    "for i in range(env.agent):\n",
    "    k = []\n",
    "    k.append(env.current_state[i][0])\n",
    "    k.append(env.current_state[i][1])\n",
    "    ab.append(k)\n",
    "UAV_positions_list.append(ab)\n",
    "UAV_positions_list = np.array(UAV_positions_list)\n",
    "# print(\"List\",UAV_positions_list)\n",
    "env.render_UAV_movement_through_episode(UAV_positions_list = UAV_positions_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13db9b9c",
   "metadata": {},
   "source": [
    "### MAPPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b867037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(optim, network, loss, grad_clip=None, retain_graph=False):\n",
    "    optim.zero_grad()\n",
    "    loss.backward(retain_graph=retain_graph)\n",
    "    if grad_clip is not None:\n",
    "        for p in network.modules():\n",
    "            torch.nn.utils.clip_grad_norm_(p.parameters(), grad_clip)\n",
    "    optim.step()\n",
    "\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    for t, s in zip(target.parameters(), source.parameters()):\n",
    "        t.data.copy_(t.data * (1.0 - tau) + s.data * tau)\n",
    "\n",
    "\n",
    "def hard_update(target, source):\n",
    "    target.load_state_dict(source.state_dict())\n",
    "\n",
    "\n",
    "def grad_false(network):\n",
    "    for param in network.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "class V(BaseNetwork):\n",
    "    def __init__(self, num_inputs, num_actions=env.action_space.shape[0], hidden_units=[256, 256],\n",
    "                 initializer='xavier'):\n",
    "        super(V, self).__init__()\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "                            nn.Linear(num_inputs, 128),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(128, 128),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(128, 1))\n",
    "    def forward(self, x):\n",
    "        q = self.value(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e029a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self,num_in,numact):\n",
    "        super(Actor,self).__init__()\n",
    "        \n",
    "        self.numact=numact\n",
    "        self.lin1 = nn.Linear(num_in,100)\n",
    "        self.lin2=nn.Linear(100,100)\n",
    "        \n",
    "        self.lin_out=nn.Linear(100,numact)\n",
    "    def forward(self,state):\n",
    "        y=T.tanh(self.lin1(state))\n",
    "        y=T.tanh(self.lin2(y))\n",
    "        y=T.tanh(self.lin_out(y))\n",
    "        return y\n",
    "    def set_std(self,new_action_std):\n",
    "        self.action_var = torch.full((self.numact,), new_action_std * new_action_std)\n",
    "        \n",
    "        \n",
    "    def get_action(self,state):\n",
    "        \n",
    "        means=self.forward(state)\n",
    "        \n",
    "        cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "        action_var = self.action_var.expand_as(means)\n",
    "        dist = MultivariateNormal(means+1e-7, cov_mat)\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action+1e-7)\n",
    "        \n",
    "        return action.detach(), action_logprob.detach()\n",
    "    def get_entropy(self,state,action):\n",
    "        means=self.forward(state)\n",
    "        action_var = self.action_var.expand_as(means)\n",
    "        cov_mat = torch.diag_embed(action_var)\n",
    "      \n",
    "        dist = MultivariateNormal(means+1e-7, cov_mat)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action+1e-7)\n",
    "        dist_entropy = dist.entropy()\n",
    " \n",
    "        \n",
    "        return action_logprobs, dist_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf57ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replaybuffer:\n",
    "    def __init__(self):\n",
    "        self.states=[]\n",
    "        self.actions=[]\n",
    "        self.rewards=[]\n",
    "        self.next_states=[]\n",
    "        self.dones=[]\n",
    "        self.logprobs=[]\n",
    "    def clear(self):\n",
    "        self.states=[]\n",
    "        self.actions=[]\n",
    "        self.rewards=[]\n",
    "        self.next_states=[]\n",
    "        self.dones=[]\n",
    "        self.logprobs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bdf904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,envstr,alpha,beta,gamma,eps_clip,K_epochs,action_std_init,min_action_std,action_std_decay_rate):\n",
    "        env =Multi_Agent_UAV()\n",
    "        self.act_dim=env.action_space.shape[0]\n",
    "        self.state_dim=env.observation_space[0].shape[0]\n",
    "#         print(self.state_dim)\n",
    "\n",
    "        self.Actor=Actor(self.state_dim,self.act_dim).to(device)\n",
    "        self.Actor_old=Actor(self.state_dim,self.act_dim).to(device)\n",
    "        self.Actor_old.load_state_dict(self.Actor.state_dict())\n",
    "        self.Value=V(env.observation_space[0].shape[0],1).to(device)\n",
    "        self.gamma=gamma\n",
    "        self.Replaybuffer=Replaybuffer()\n",
    "        self.optimizer=T.optim.Adam([{'params': self.Actor.parameters(), 'lr': alpha},\n",
    "                        {'params': self.Value.parameters(), 'lr': beta}])\n",
    "        self.MseLoss=nn.MSELoss()\n",
    "        self.grad_clip=1\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.action_std=action_std_init\n",
    "        self.min_action_std=min_action_std\n",
    "        self.action_std_decay_rate=action_std_decay_rate\n",
    "        self.Actor.set_std(self.action_std)\n",
    "        self.grad_clip=1\n",
    "    def get_action(self,state):\n",
    "   \n",
    "        state=T.FloatTensor(np.array(state).reshape(-1,self.state_dim)).to(device)\n",
    "\n",
    "   \n",
    "        action,logprob=self.Actor.get_action(state)\n",
    "        \n",
    "        self.Replaybuffer.logprobs.append(logprob)\n",
    "        self.Replaybuffer.states.append(state)\n",
    "        self.Replaybuffer.actions.append(action)\n",
    "        \n",
    "        return action.detach().cpu().numpy()[0]\n",
    "    def decay_std(self):\n",
    "        self.action_std = self.action_std - self.action_std_decay_rate\n",
    "        self.action_std = round(self.action_std, 4)\n",
    "        if (self.action_std <= self.min_action_std):\n",
    "            self.action_std = self.min_action_std\n",
    "        \n",
    "        self.Actor.set_std(self.action_std)\n",
    "            \n",
    "    def update(self):\n",
    "\n",
    "\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, done in zip(reversed(self.Replaybuffer.rewards), reversed(self.Replaybuffer.dones)):\n",
    "            if done:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "            \n",
    "\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        old_states = torch.squeeze(torch.stack(self.Replaybuffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.Replaybuffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.Replaybuffer.logprobs, dim=0)).detach().to(device)\n",
    "\n",
    "\n",
    "        \n",
    "        for _ in range(self.K_epochs):\n",
    "            \n",
    "            values=self.Value(old_states)\n",
    "  \n",
    "            logprobs, dist_entropy = self.Actor.get_entropy(old_states, old_actions)\n",
    "\n",
    "       \n",
    "            state_values = torch.squeeze(values)\n",
    "            \n",
    "   \n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "  \n",
    "            advantages = rewards - state_values.detach()   \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "           \n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            for p in self.Actor.modules():\n",
    "                torch.nn.utils.clip_grad_norm_(p.parameters(), self.grad_clip)\n",
    "            for p in self.Value.modules():\n",
    "                torch.nn.utils.clip_grad_norm_(p.parameters(),self.grad_clip)\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.Actor_old.load_state_dict(self.Actor.state_dict())\n",
    "\n",
    "\n",
    "        self.Replaybuffer.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32a53ac",
   "metadata": {},
   "source": [
    "### Creating Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de91ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Agent(string,lr_actor,lr_critic,gamma,eps_clip,K_epochs,action_std,min_action_std,action_std_decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6895478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent= [copy.deepcopy(a) for i in range(env.agent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0468fd",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db2cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=Multi_Agent_UAV()\n",
    "step=0\n",
    "rp=0\n",
    "scores=[]\n",
    "\n",
    "# uav_energy_array=[]\n",
    "# ue_energy_array=[]\n",
    "# aoi_array=[]\n",
    "time_per_ep=[]\n",
    "\n",
    "while rp<=max_episodes:\n",
    "    # pos_list=[]\n",
    "    interscore=0\n",
    "    start_time=time.time()\n",
    "        \n",
    "    state = env.reset()\n",
    "    done=False\n",
    "    current_ep_reward = 0\n",
    "\n",
    "    for j in range(max_steps):\n",
    "        step+=1\n",
    "        action = [agent[i].get_action(state[i]) for i in range(env.agent)]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        interscore+=reward\n",
    "\n",
    "        for i in range(env.agent):\n",
    "            agent[i].Replaybuffer.rewards.append(reward)\n",
    "            agent[i].Replaybuffer.dones.append(done)\n",
    "\n",
    "        # update PPO agent\n",
    "            if step % 4000 == 0:\n",
    "                agent[i].update()\n",
    "\n",
    "\n",
    "            if step % 100000 == 0:\n",
    "                agent[i].decay_std()\n",
    "        scores.append(interscore)\n",
    "        \n",
    "        # pos_list.append([i[0],i[1]] for i in env.current_state)\n",
    "        if done:\n",
    "            rp+=1\n",
    "            time_per_ep.append(time.time()-start_time)\n",
    "            # print(f\"episode time : {time_per_ep[time_per_ep.__len__()-1]}\")\n",
    "            # store output values\n",
    "            # uav_energy_array.append(env.UAV_energy)\n",
    "            # ue_energy_array.append(env.UE_energy)\n",
    "            # aoi_array.append(env.AOI)\n",
    "            # print(\"episode done\")\n",
    "            \n",
    "            break\n",
    "\n",
    "    # rp+=1\n",
    "    if rp%40==0:\n",
    "        print(np.mean(scores[-100:]),step,\" episodes : \",rp)\n",
    "        # env.render_UAV_movement_through_episode(UAV_positions_list=UAV_positions_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8903417",
   "metadata": {},
   "source": [
    "### Render UAV position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "score = 0\n",
    "UAV_positions_list =[]\n",
    "while not done:\n",
    "    action = [agent[i].get_action(state[i]) for i in range(env.agent)]\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    score += reward\n",
    "    ab=[]\n",
    "    for i in range(env.agent):\n",
    "        k = []\n",
    "        k.append(env.current_state[i][0])\n",
    "        k.append(env.current_state[i][1])\n",
    "        ab.append(k)\n",
    "    UAV_positions_list.append(ab)\n",
    "\n",
    "    # print('score %.1f' % score, 'avg_score %.1f' % score)\n",
    "UAV_positions_list = np.array(UAV_positions_list)\n",
    "            # print(UAV_positions_list)\n",
    "env.render_UAV_movement_through_episode(UAV_positions_list = UAV_positions_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ed82c",
   "metadata": {},
   "source": [
    "### Result plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f12f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data={'scores':scores})\n",
    "df2=df.copy()\n",
    "df2['max']=df2.scores.rolling(100).mean()\n",
    "df2.drop(columns=['scores'],inplace=True)\n",
    "\n",
    "df['mean']=df.scores.rolling(100).mean()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c78cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(figsize=(18,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e24a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_mean(a,shift):\n",
    "    cur_mean=np.mean(a[:shift])\n",
    "    result=[cur_mean]\n",
    "    \n",
    "    \n",
    "    for i in range(len(a)-shift):\n",
    "        cur_mean+=(a[i+shift]-a[i])/shift\n",
    "        result.append(cur_mean)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d5bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "utility=calculate_rolling_mean(list(df2.loc[:,'max'].dropna()),500)\n",
    "plt.plot(range(len(utility)),utility)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"avg. utility\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1546d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(time_per_ep)),time_per_ep)\n",
    "plt.xlabel(\"episodes\")\n",
    "plt.ylabel(\"time taken\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3986452",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_plot=[np.mean(i) for i in env.energy_array]\n",
    "shift=500\n",
    "energy_plot=calculate_rolling_mean(energy_plot,shift)\n",
    "plt.plot(range(len(energy_plot)),energy_plot)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"avg. UAV energy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3acc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi_plot=[np.mean(i) for i in env.AOI_array]\n",
    "shift=500\n",
    "aoi_plot=calculate_rolling_mean(aoi_plot,shift)\n",
    "plt.plot(range(len(aoi_plot)),aoi_plot)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"avg. AOI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7f6060",
   "metadata": {},
   "source": [
    "### Save results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b23a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "result_count=\"MAPPO_DBSCAN\"\n",
    "save_to_file=True\n",
    "\n",
    "if save_to_file:\n",
    "    with open(f\"./results/result_{result_count}_utility.json\",\"w\") as f:\n",
    "        f.write(json.dumps(utility))\n",
    "    with open(f\"./results/result_{result_count}_UAV_energy.json\",\"w\") as f:\n",
    "        f.write(json.dumps(env.energy_array))\n",
    "    with open(f\"./results/result_{result_count}_AOI.json\",\"w\") as f:\n",
    "        f.write(json.dumps(env.AOI_array))\n",
    "    with open(f\"./results/result_{result_count}_pos.json\",\"w\") as f:\n",
    "        f.write(json.dumps(UAV_positions_list.tolist()))\n",
    "    with open(f\"./results/result_{result_count}_time_per_ep.json\",\"w\") as f:\n",
    "        f.write(json.dumps(time_per_ep))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
