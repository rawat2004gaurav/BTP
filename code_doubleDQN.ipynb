{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78591917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import math\n",
    "import pandas as pd\n",
    "from gym import wrappers\n",
    "from collections import deque\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(action='ignore', category=DeprecationWarning, message='`np.bool` is a deprecated alias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007804c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from collections import deque\n",
    "import gym\n",
    "\n",
    "import os \n",
    "import numpy as np\n",
    "from random import sample\n",
    "import torch as T \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributions import Categorical,Normal,MultivariateNormal\n",
    "from multiprocessing.pool import Pool\n",
    "import random\n",
    "from rltorch.network import create_linear_network,BaseNetwork\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "# torch.set_grad_enabled(True)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7facef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "num_iotds = 100 #number\n",
    "X_min = 0 #meters\n",
    "X_max = 200 #meters\n",
    "Y_min = 0 #meters\n",
    "Y_max = 200 #meters\n",
    "O = 9 #number\n",
    "R_obstacles_max = 10 #meters\n",
    "Safe_radius = 2 #meters\n",
    "Cluster_radius = 20 #meters\n",
    "H = 50 #meters\n",
    "E_wake = 0.1 #Watt/s\n",
    "Comm_radius = 10 #meters\n",
    "Wake_radius = 30 #meters\n",
    "B = 500 * 10**6 # hertz , available channel bandwidth\n",
    "beta_o = 31.62 # Watts, path loss at reference distance\n",
    "rho_m = 0.1 # Watts, transmitting power of a CH\n",
    "sigma = 10**-13 # Watts , noise power\n",
    "alpha = 2 # path loss exponent\n",
    "V_max = 10 #m/s\n",
    "V_min = 5 #m/s\n",
    "E_start_UAV = 1000 #Joules\n",
    "E_hover = 100 #Watt\n",
    "U_tip = 200 #m/s\n",
    "v_o = 10 # m/s , mean rotor induced velocity in the hovering state\n",
    "rho = 1.225 # kg/m^3, air density\n",
    "delta = 0.202 # rotor area\n",
    "d_o = 0.1 # fusealage drag ratio , dimensionless\n",
    "s = 0.1 # rotor solidity , dimensionless\n",
    "P_o = 0.7 # blade profile power coefficient , dimensionless\n",
    "P_i = 0.2 # induced power coefficient in the hovering state \n",
    "P_h = 100 #Watt, power required for hovering\n",
    "D_max = 10 # meters\n",
    "D_min = 1 # meters\n",
    "E_IoTD_start = 1000000 #Joules\n",
    "Distance_O_penalty = 100\n",
    "Distance_multiplier = 0.1\n",
    "h_reward = 100000\n",
    "\n",
    "k_E=0.1\n",
    "k_AOI=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a842e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import sqrt\n",
    "\n",
    "def calculate_distance(coord1, coord2):\n",
    "    distance = sqrt((coord1[0] - coord2[0])**2 + (coord1[1] - coord2[1])**2)\n",
    "    return distance\n",
    "\n",
    "def calculate_UAV_CH_distance(P_t,P_m):\n",
    "    distance = sqrt((P_t[0] - P_m[0])**2 + (P_t[1] - P_m[1])**2 + P_t[2]**2)\n",
    "    return distance\n",
    "\n",
    "def calculate_power_gain_UAV_CH(beta_o,P_t,P_m):\n",
    "    distance = calculate_UAV_CH_distance(P_t,P_m)\n",
    "    power_gain = beta_o/(distance**2)\n",
    "    return power_gain\n",
    "\n",
    "def calculate_UAV_CH_transmission_rate(B, P_t, P_m, beta_o, sigma, rho_m):\n",
    "    power_gain = calculate_power_gain_UAV_CH(beta_o, P_t, P_m)\n",
    "    transmission_rate = B * np.log2(1 + (rho_m * power_gain)/(sigma**2))\n",
    "    return transmission_rate\n",
    "\n",
    "def calculate_UAV_CH_Upload_time(B, P_t, P_m, beta_o, sigma, rho_m, I_m):\n",
    "    transmission_rate = calculate_UAV_CH_transmission_rate(B, P_t, P_m, beta_o, B , sigma, rho_m)\n",
    "    upload_time = I_m/transmission_rate\n",
    "    return upload_time\n",
    "\n",
    "def calculate_CH_IoTD_distance(P_m, P_n):\n",
    "    distance = sqrt((P_m[0] - P_n[0])**2 + (P_m[1] - P_n[1])**2)\n",
    "    return distance\n",
    "\n",
    "def calculate_CH_IOTD_transmission_rate(B, P_m, P_n, beta_m,rho_n,alpha , rho_i):\n",
    "    distance = calculate_CH_IoTD_distance(P_m, P_n)\n",
    "    power_gain = beta_m/(distance**alpha)\n",
    "    transmission_rate = B * np.log2(1 + (rho_i * power_gain)/(rho_n))\n",
    "    return transmission_rate\n",
    "\n",
    "def calculate_UAV_Obstacle_distance(P_t,L_o):\n",
    "    distance = sqrt((P_t[0]-L_o[0])**2 + (P_t[1]-L_o[1])**2) - L_o[2]\n",
    "    return distance\n",
    "\n",
    "def calculate_UAV_travel_energy_per_second(P_o,v_t,U_tip,P_i,v_o,d_o,rho,s,delta):\n",
    "    energy_per_second = P_o*(1+3*(v_t**2)/(U_tip**2)) + P_i*v_o/v_t + d_o*rho*s*delta*(v_t**3)/2\n",
    "    return energy_per_second\n",
    "\n",
    "def maximum_minimum_distance_clustering(nodes, cluster_radius):\n",
    "    cluster_heads = []\n",
    "    remaining_nodes = nodes.copy()\n",
    "\n",
    "    first_node=remaining_nodes.pop(0)\n",
    "    \n",
    "    cluster_heads.append((first_node[0],first_node[1]))\n",
    "\n",
    "    while remaining_nodes:\n",
    "        max_min_distance = 0\n",
    "        max_min_distance_node = None\n",
    "        for node in remaining_nodes:\n",
    "            min_distance = min(calculate_distance(node, ch) for ch in cluster_heads)\n",
    "            if min_distance > max_min_distance and min_distance > cluster_radius:\n",
    "                max_min_distance = min_distance\n",
    "                max_min_distance_node = node\n",
    "        if max_min_distance_node is None:\n",
    "            break\n",
    "        cluster_heads.append((max_min_distance_node[0],max_min_distance_node[1]))\n",
    "        remaining_nodes.remove(max_min_distance_node)\n",
    "\n",
    "    # print(cluster_heads)\n",
    "    clusters = {ch: [] for ch in cluster_heads}\n",
    "    for node in remaining_nodes:\n",
    "        nearest_ch = min(cluster_heads, key=lambda ch: calculate_distance(node, ch))\n",
    "        clusters[nearest_ch].append(node)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def generate_IoTD_coordinates_outside_obstacles(num_coordinates, Obstacle_coordinates):\n",
    "    coordinates = []\n",
    "    xmin = X_min\n",
    "    xmax = X_max\n",
    "    ymin = Y_min\n",
    "    ymax = Y_max\n",
    "    for i in range(num_coordinates):\n",
    "        x = random.uniform(xmin, xmax)\n",
    "        y = random.uniform(ymin, ymax)\n",
    "        new_coord = (x, y)\n",
    "        if not any(calculate_distance(new_coord, obstacle) <= obstacle[2] + Safe_radius for obstacle in Obstacle_coordinates):\n",
    "            coordinates.append(new_coord)\n",
    "    \n",
    "    return coordinates\n",
    "\n",
    "    \n",
    "\n",
    "def generate_Obstacle_coordinates(num_coordinates):\n",
    "    coordinates = []\n",
    "    xmin = X_min\n",
    "    xmax = X_max\n",
    "    ymin = Y_min\n",
    "    ymax = Y_max\n",
    "    for i in range(num_coordinates):\n",
    "        x = random.uniform(xmin, xmax)\n",
    "        y = random.uniform(ymin, ymax)\n",
    "        r = random.uniform(0, R_obstacles_max)\n",
    "        new_coord = (x, y, r)\n",
    "        coordinates.append(new_coord)\n",
    "    \n",
    "    return coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c2ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from math import sqrt\n",
    "\n",
    "coordinates_file_name=\"coordinates.csv\"\n",
    "\n",
    "\n",
    "def calculate_distance(coord1, coord2):\n",
    "    distance = sqrt((coord1[0] - coord2[0])**2 + (coord1[1] - coord2[1])**2)\n",
    "#     print(distance)\n",
    "    return distance\n",
    "\n",
    "def save_to_csv(filename, coordinates):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['X', 'Y'])\n",
    "        csv_writer.writerows(coordinates)\n",
    "\n",
    "X=[(random.randint(X_min,X_max),random.randint(Y_min,Y_max)) for _ in range(num_iotds)]\n",
    "print(X)\n",
    "save_to_csv(coordinates_file_name, X)\n",
    "\n",
    "print(f\"{num_iotds} coordinates generated and saved to {coordinates_file_name}.\")\n",
    "def distance(u,v,w,x,y,z):\n",
    "    distance = ((u-x)**2 + (v-y)**2 + (w-z)**2)**0.5\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b173a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "O_obstacles = [\n",
    "    [50,50,10],\n",
    "    [100,50,10],\n",
    "    [150,50,10],\n",
    "    [50,100,10],\n",
    "    [100,100,10],\n",
    "    [150,100,10],\n",
    "    [50,150,10],\n",
    "    [100,150,10],\n",
    "    [150,150,10]\n",
    "]\n",
    "N = generate_IoTD_coordinates_outside_obstacles(100, O_obstacles)\n",
    "# N=[\n",
    "#     [25,75],\n",
    "#     [25,125],\n",
    "#     [75,75],\n",
    "#     [75,125],\n",
    "#     [125,75],\n",
    "#     [125,125],\n",
    "#     [175,75],\n",
    "#     [175,125],\n",
    "#     [75,25],\n",
    "#     [125,25],\n",
    "#     [75,175],\n",
    "#     [125,175],\n",
    "    \n",
    "# ] \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot IoTDs\n",
    "# for coord in N:\n",
    "#     plt.scatter(coord[0], coord[1], color='blue', label='IoTD')\n",
    "    \n",
    "# Plot Obstacles\n",
    "# for obstacle in O_obstacles:\n",
    "#     circle = plt.Circle((obstacle[0], obstacle[1]), obstacle[2], color='red', fill=False)\n",
    "#     plt.gca().add_patch(circle)\n",
    "\n",
    "# Set plot limits\n",
    "# plt.xlim(X_min - 10, X_max + 10)\n",
    "# plt.ylim(Y_min - 10, Y_max + 10)\n",
    "\n",
    "# Add legend\n",
    "# plt.legend()\n",
    "# plt.gca().set_aspect('equal')\n",
    "\n",
    "# Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c197b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=N\n",
    "Clusters = maximum_minimum_distance_clustering(X, Cluster_radius)\n",
    "X=list(Clusters.keys())\n",
    "print(X)\n",
    "M = []\n",
    "\n",
    "for cluster_head, cluster_nodes in Clusters.items():\n",
    "    M.append(cluster_head)\n",
    "\n",
    "M=X.copy()\n",
    "\n",
    "x,y=[node[0] for node in Clusters.keys()],[node[1] for node in Clusters.keys()]\n",
    "for i in Clusters.keys():\n",
    "    color=(random.choice(range(256))/255,\n",
    "           random.choice(range(256))/255,\n",
    "           random.choice(range(256))/255)\n",
    "    plt.scatter(i[0],i[1],color=color,marker='*')\n",
    "    for j in Clusters[i]:\n",
    "        plt.scatter(j[0],j[1],color=color)\n",
    "for obstacle in O_obstacles:\n",
    "    circle = plt.Circle((obstacle[0], obstacle[1]), obstacle[2], color='red', fill=False)\n",
    "    plt.gca().add_patch(circle)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()\n",
    "print(len(x))\n",
    "Data = []\n",
    "\n",
    "for cluster_head , cluster_nodes in Clusters.items():\n",
    "    Data_cluster = 0\n",
    "    for node in cluster_nodes:\n",
    "        Data_cluster += random.randint(1, 10)\n",
    "    Data.append(Data_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Agent_UAV(gym.Env):\n",
    "    def __init__(self, initial_UAV_state = None):\n",
    "        super(Multi_Agent_UAV, self).__init__()\n",
    "\n",
    "        self.agent = 2\n",
    "        self.pos_list = []\n",
    "        self.color = ['red', 'green', 'blue', 'cyan', 'magenta']\n",
    "        self.initial_UAV_state = np.array([100,100])\n",
    "        self.UE_count = len(X)\n",
    "        self.Z = 20\n",
    "        self.max_angle = 2 * math.pi\n",
    "        self.phi_n = np.radians(42.44)  # in degrees\n",
    "        self.C_max_t = (self.Z / np.tan(self.phi_n))\n",
    "        self.boundary_x = self.boundary_y = 200.\n",
    "        self.max_episode_steps = 30 # Maximum number of steps in a single episode, after which environment returns done = True\n",
    "        self._max_episode_steps = 30\n",
    "        self.current_episode_timestep = 0\n",
    "        self.done = False\n",
    "        self.visited = np.array([0 for i in range(self.UE_count)])\n",
    "        self.state_space_lb = np.array([0, 0], dtype = np.float32)\n",
    "        self.state_space_ub = np.array([self.boundary_x, self.boundary_y], dtype = np.float32)\n",
    "        self.energy_array=[]\n",
    "        self.AOI_array=[]\n",
    "        \n",
    "        # plotting parameters\n",
    "        self.plotting_boundary_buffer = 5.\n",
    "        self.UAV_coverage_circle_color = 'lavender'\n",
    "        self.UAV_path_color = 'green'\n",
    "        obh = []\n",
    "        obl = []\n",
    "        self.weight = []\n",
    "        self.visited = np.array([0.0 for i in range(self.UE_count)])\n",
    "        \n",
    "        # AOI\n",
    "        # obh.append(float('inf'))\n",
    "        # obl.append(0)\n",
    "        \n",
    "        # UAV Energy\n",
    "        # obh.append(E_start_UAV)\n",
    "        # obl.append(0)\n",
    "        for i in range(len(X)+6+self.UE_count):\n",
    "            obh.append(float('inf')) \n",
    "        for i in range(len(X)+6+self.UE_count):\n",
    "            obl.append(float('-inf'))\n",
    "        \n",
    "        \n",
    "        high = np.array(obh, dtype=np.float32)\n",
    "        low = np.array(obl, dtype=np.float32)\n",
    "        high1 = np.array([len(X), 180.0, 6.0], dtype=np.float32)\n",
    "        low1 = np.array([0, -180.0, 5.0], dtype=np.float32)\n",
    "        \n",
    "        self.observation_space = np.array([spaces.Box(low, high) for i in range(self.agent)])\n",
    "        #self.action_space = spaces.Box(low=low1, high=high1, dtype=np.float32)\n",
    "        self.action_space = spaces.Box(-1, 1, (2,), dtype=np.float32)\n",
    "\n",
    "        self.place_UEs(position=\"random\")\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        # RANDOMLY SELECTS UAV STARTING POSITION\n",
    "        self.current_state = self.select_random_state()\n",
    "        ground_UAV_state = self.current_state[:4]\n",
    "        self.visited = np.array([0 for i in range(self.UE_count)])\n",
    "        self.timesteps_in_episode = 0\n",
    "        self.done = False\n",
    "        self.c_state = []\n",
    "        self.AOI=[0 for _ in range(self.UE_count)]\n",
    "        self.UAV_energy=[E_start_UAV for _ in range(self.agent)]\n",
    "        for j in range(self.agent):\n",
    "            obs = []\n",
    "            obs.append(j)\n",
    "            obs.append(self.current_state[j][0].copy()/1000)\n",
    "            obs.append(self.current_state[j][1].copy()/1000)\n",
    "            obs.append(self.current_state[j][0].copy()/1000)\n",
    "            obs.append(self.current_state[j][1].copy()/1000)\n",
    "            for i in range(len(X)):\n",
    "                obs.append(-(math.dist(self.UE_positions[i], ground_UAV_state[j]))/1000)\n",
    "            obs.append(self.UAV_energy[j]/2000)\n",
    "            obs+=[i/200 for i in self.AOI]\n",
    "            self.c_state.append(obs)\n",
    "        self.c_state = np.array(self.c_state)\n",
    "        return self.c_state\n",
    "    \n",
    "    \n",
    "    def get_count_of_UEs_covered(self):\n",
    "        profit = 0\n",
    "        ground_UAV_state = self.current_state[:self.agent]\n",
    "        for j in range(self.agent):\n",
    "            for i in range(self.UE_count):\n",
    "                if(math.dist(self.UE_positions[i], ground_UAV_state[j])<=self.C_max_t):\n",
    "                    if(self.visited[i]==0):\n",
    "                        profit = profit + 500\n",
    "                        self.visited[i] =1\n",
    "                        self.AOI[i]=0\n",
    "                    else:\n",
    "                        profit = profit-100\n",
    "                        self.AOI[i]+=1\n",
    "                else:\n",
    "                    if(self.visited[i]==0):\n",
    "                        profit = profit -1\n",
    "                        self.AOI[i]+=1\n",
    "#         horizontal_dist_UE_UAV = np.linalg.norm(self.UE_positions - ground_UAV_state, axis = 1)\n",
    "        # print(\"horizontal distance b/w UEs and UAV: \", horizontal_dist_UE_UAV)\n",
    "#         rho_array = (horizontal_dist_UE_UAV <= (self.C_max_t)) * 1  # binary association vector\n",
    "#         M_t = rho_array.sum()\n",
    "        return profit\n",
    "\n",
    "\n",
    "    def render(self, **kwargs):\n",
    "        '''\n",
    "        Function to visualize UAV position\n",
    "        (this function has different configurations for different use cases)\n",
    "        '''\n",
    "        if 'UAV_positions_list' in kwargs:\n",
    "            self.render_UAV_movement_through_episode(UAV_positions_list = kwargs['UAV_positions_list'])\n",
    "\n",
    "        if 'fig' not in kwargs:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "            ax = np.array([ax])\n",
    "            kwargs['fig'] = fig\n",
    "            kwargs['ax'] = ax\n",
    "            kwargs['i'] = 0\n",
    "        \n",
    "        if 'reward' not in kwargs:\n",
    "            kwargs['reward'] = 'Not known'\n",
    "        self.render_position_plot(**kwargs)\n",
    "\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "        '''\n",
    "        Takes action moving environment from current_state to next_state\n",
    "        Arguments: `action` to be taken\n",
    "        Returns: new_state, reward, done, info(=None)\n",
    "        '''\n",
    "        # action = action.numpy().flatten()\n",
    "#         err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "#         assert self.action_space.contains(action), err_msg\n",
    "#         assert self.current_state is not None, \"Call reset before using step method.\"\n",
    "        self.timesteps_in_episode += 1\n",
    "        prev_UAV_state = self.current_state.copy()\n",
    "        _, inside_bounds = self.move(action)\n",
    "        \n",
    "        # print(f\"prev state : {prev_UAV_state} new state : {self.current_state}\")\n",
    "        energy_used=[0 for _ in range(self.agent)]\n",
    "        for i in range(self.agent):\n",
    "            velocity = (action[i][0]+1.5)*5 + 2.5  # velocity of the UAV\n",
    "            velocity = v_o\n",
    "            step_size = (action[i][0]+1.2)*10    # step size of the UAV\n",
    "            angle = (action[i][1])*np.pi  # angle of the UAV\n",
    "            \n",
    "            time_flew = calculate_distance(prev_UAV_state[i], self.current_state[i])/velocity\n",
    "            # Calculating the energy used by the UAV in the current step\n",
    "            energy_used[i] = time_flew * calculate_UAV_travel_energy_per_second(P_o, velocity, U_tip, P_i, v_o, d_o, rho, s, delta)\n",
    "            \n",
    "            self.UAV_energy[i]-=energy_used[i]\n",
    "        \n",
    "        \n",
    "        # print(type(self.current_state), type(prev_UAV_state))\n",
    "#         if not inside_bounds:\n",
    "#             MENTIONED OUTPUT IS OUT OF BOUNDS\n",
    "#             self.is_done(end = True)\n",
    "#             reward = -10000\n",
    "#             return self.current_state, reward, self.done, None\n",
    "\n",
    "#         ground_UAV_state = self.current_state[:2]\n",
    "#         horizontal_dist_UE_UAV = np.linalg.norm(self.UE_positions - ground_UAV_state, axis = 1)\n",
    "#         rho_array = (horizontal_dist_UE_UAV <= (self.C_max_t)) * 1  # binary association vector\n",
    "#         # self.current_state = np.concatenate((self.current_state[:2], rho_array))\n",
    "#         M_t = rho_array.sum()  # no. of UEs served by the agent\n",
    "\n",
    "        reward = self.get_count_of_UEs_covered()\n",
    "        \n",
    "        # print(f\"AOI : {self.AOI}\")\n",
    "        \n",
    "        for i in range(self.agent):\n",
    "            if self.UAV_energy[i]<0:\n",
    "                reward-=10000\n",
    "            else:\n",
    "                reward-=k_E*energy_used[i]\n",
    "        avg_AOI=sum(self.AOI)/len(self.AOI)\n",
    "        reward-=k_AOI*avg_AOI\n",
    "        # reward-=50\n",
    "        \n",
    "        # for i in range(self.UE_count):\n",
    "        #     reward-=k_AOI*self.AOI[i]\n",
    "        \n",
    "#         if(sum(self.visited)>15):\n",
    "#             reward = 100 * sum(self.visited) \n",
    "        self.is_done(sum(self.visited))\n",
    "        if self.is_done:\n",
    "            self.energy_array.append(self.UAV_energy)\n",
    "            self.AOI_array.append(avg_AOI)\n",
    "        self.c_state=[]\n",
    "        ground_UAV_state = self.current_state[:self.agent]\n",
    "        for j in range(self.agent):\n",
    "            obs = []\n",
    "            obs.append(j)\n",
    "            obs.append(self.current_state[j][0].copy()/1000)\n",
    "            obs.append(self.current_state[j][1].copy()/1000)\n",
    "            obs.append(prev_UAV_state[j][0].copy()/1000)\n",
    "            obs.append(prev_UAV_state[j][1].copy()/1000)\n",
    "            for i in range(len(X)):\n",
    "                if(self.visited[i] == 0):\n",
    "                    obs.append(-(math.dist(self.UE_positions[i], ground_UAV_state[j]))/10000)\n",
    "                else:\n",
    "                    obs.append(1)\n",
    "            obs.append(self.UAV_energy[j]/2000)\n",
    "            obs+=[i/200 for i in self.AOI]\n",
    "            self.c_state.append(obs)\n",
    "        self.c_state = np.array(self.c_state)\n",
    "        for i in range(self.agent):\n",
    "            for j in range(self.agent):\n",
    "                if(i!=j):\n",
    "                    dis = distance(self.current_state[i][0],self.current_state[i][1], self.current_state[j][0], self.current_state[j][1],0,0)\n",
    "                    if(dis<10):\n",
    "                        reward = reward -10000\n",
    "        return self.c_state, reward, self.done, None\n",
    "    \n",
    "\n",
    "    def move(self, action: np.ndarray):\n",
    "        '''\n",
    "        Helper function to step() function.\n",
    "        Clips the passed action to fit within action space bounds.\n",
    "        Calculates new state after performing the passed action, and updates UAV position accordingly. \n",
    "        '''\n",
    "#         evaluates new state reached upon performing the move and saves it in self.current_state\n",
    "        # ACTION = [dx, dy]\n",
    "#         angle1= math.degrees(math.atan(action))\n",
    "#         print(angle1)\n",
    "        for i in range(self.agent):\n",
    "            self.action_step_size = (action[i][0]+1.5)*10\n",
    "            angle1 = action[i][1] * 180\n",
    "            angle = angle1*math.pi/180\n",
    "            x_next = self.current_state[i][0] + (math.cos(angle) * self.action_step_size)\n",
    "            y_next = self.current_state[i][1] + (math.sin(angle) * self.action_step_size)\n",
    "            # updating horizontal_direction_angle if the new move is out of boundary\n",
    "            if((x_next < 0) or (x_next > self.boundary_x) or (y_next < 0) or (y_next > self.boundary_y)):\n",
    "            # UAV REMAINS IN ITS CURRENT POSITION\n",
    "                return action, False\n",
    "\n",
    "            self.current_state[i][:2] = np.array([x_next, y_next])\n",
    "        return action, True\n",
    "\n",
    "\n",
    "    def is_done(self, M_t=0, end=False):\n",
    "        '''\n",
    "        Helper function to check if episode needs to be terminated\n",
    "        '''\n",
    "        if end == True:\n",
    "            self.done = True\n",
    "        elif(sum(self.visited)==len(X)):\n",
    "            print(sum(self.visited))\n",
    "            self.done = True\n",
    "        elif(self.timesteps_in_episode >= self.max_episode_steps):\n",
    "            self.done = True\n",
    "        return\n",
    "\n",
    "    \n",
    "    def select_random_state(self):\n",
    "        '''\n",
    "        Selects (and returns) random initial state (within bounds) for the UAV\n",
    "        '''\n",
    "        UAV_pos_list = []\n",
    "        for _ in range(self.agent):         \n",
    "            new_x = 100.0\n",
    "            new_y = 100.0\n",
    "\n",
    "            UAV_pos_list.append(np.array([new_x, new_y]))\n",
    "\n",
    "        return np.array(UAV_pos_list)\n",
    "\n",
    "    def place_UEs_randomly(self, random_UE_count = None):\n",
    "        '''\n",
    "        Helper function to place_UEs\n",
    "        places `random_UE_count` UEs randomly onto the rectangular region\n",
    "        if kwargs has the key `exclude_center`, then the circular region spanned by `exclude_center` \\\n",
    "        and `exclude_radius` is excluded\n",
    "        '''\n",
    "\n",
    "        if random_UE_count is None:\n",
    "          random_UE_count = self.UE_count\n",
    "\n",
    "        # places UE_count UEs on grid randomly\n",
    "        if type(self.UE_positions) == np.ndarray:\n",
    "            self.UE_positions = self.UE_positions.tolist()\n",
    "\n",
    "        randomly_placed_count = 0\n",
    "        while(randomly_placed_count < random_UE_count):\n",
    "            x = X[randomly_placed_count][0]\n",
    "            y = X[randomly_placed_count][1]\n",
    "            coords = np.array([x, y])\n",
    "            self.UE_positions.append(coords)\n",
    "            randomly_placed_count += 1\n",
    "        \n",
    "\n",
    "        self.UE_positions = np.array(self.UE_positions)\n",
    "\n",
    "\n",
    "    def place_UEs(self, position=\"random\"):\n",
    "        '''\n",
    "        Function to place UEs onto the rectangular region\n",
    "        '''\n",
    "        self.UE_positions = np.zeros((0, 2))\n",
    "        self.place_UEs_randomly(self.UE_count)\n",
    "\n",
    "        # saves in self.UE_positions\n",
    "        self.UE_positions = np.array(self.UE_positions)\n",
    "        return\n",
    "    def render_UAV_movement_through_episode(self, **kwargs):\n",
    "        '''\n",
    "        Helper function to visualize UAV movement through an episode\n",
    "        Arguments: A list specifying UAV positions throughout the episode\n",
    "        '''\n",
    "\n",
    "        if 'reward' not in kwargs:\n",
    "            kwargs['reward'] = 'Not known'\n",
    "        \n",
    "        buffer = 5.\n",
    "        fig, ax = plt.subplots(1, figsize=(10,10))\n",
    "        ax.set_xlim(-buffer, self.boundary_x+buffer)\n",
    "        ax.set_ylim(-buffer, self.boundary_y+buffer)\n",
    "        ax.grid()\n",
    "        \n",
    "        for j in range(len(X)):\n",
    "            UE_x1 = X[j][0]\n",
    "            UE_y1 = X[j][1]\n",
    "            if(self.visited[j] == 0):\n",
    "                ax.plot(UE_x1, UE_y1, color=self.color[0], marker='o', markersize=6, linestyle = '') \n",
    "            else:\n",
    "                ax.plot(UE_x1, UE_y1, color=self.color[1], marker='o', markersize=6, linestyle = '')\n",
    "        \n",
    "        for i in range(self.agent):\n",
    "            ax.plot(kwargs['UAV_positions_list'][:, i, 0] , kwargs['UAV_positions_list'][:, i, 1], color=self.UAV_path_color, markersize=6, linestyle = '-') #, label = \"UAV Path Color\")\n",
    "            ax.plot(kwargs['UAV_positions_list'][-1, i, 0], kwargs['UAV_positions_list'][-1, i, 1], color='blue', marker='x', markersize=9, linestyle = '') #, label = \"UAV ending position\")\n",
    "            \n",
    "            UAV_coverage_area = plt.Circle((kwargs['UAV_positions_list'][-1, i, 0], kwargs['UAV_positions_list'][-1, i, 1]), self.C_max_t, color = self.UAV_coverage_circle_color)\n",
    "            ax.add_artist(UAV_coverage_area)\n",
    "\n",
    "        ax.set_aspect(1)\n",
    "        # naming the x axis\n",
    "        ax.set_xlabel('X pos (m)')\n",
    "        # naming the y axis\n",
    "        ax.set_ylabel('Y pos (m)')\n",
    "        ax.set_title('UAV movement through episode')\n",
    "        # giving a title to my graph\n",
    "        # plt.title('Visually Appealing!')\n",
    "\n",
    "        # show a legend on the plot\n",
    "        ax.legend()\n",
    "        # plt.show()\n",
    "        return\n",
    "    \n",
    "    def render_position_plot(self, **kwargs):\n",
    "        '''\n",
    "        Helper function to render(), plots the current position plot on given axes. \n",
    "        Plotting position plot of UAV's current position and coverage\n",
    "        '''\n",
    "        ax = kwargs['ax']\n",
    "        plot_number = kwargs['i']\n",
    "        idx = 0\n",
    "        reward = kwargs['reward']\n",
    "\n",
    "        buffer = 5.\n",
    "        ax[plot_number].set_xlim(-buffer, self.boundary_x+buffer)\n",
    "        ax[plot_number].set_ylim(-buffer, self.boundary_y+buffer)\n",
    "        ax[plot_number].grid()\n",
    "        for j in range(len(X)):\n",
    "            UE_x1 = X[j][0]\n",
    "            UE_y1 = X[j][1]\n",
    "            if(self.visited[j] ==0):\n",
    "                ax[plot_number].plot(UE_x1, UE_y1, color=self.color[0], marker='o', markersize=6, linestyle = '')\n",
    "            else:\n",
    "                ax[plot_number].plot(UE_x1, UE_y1, color=self.color[1], marker='o', markersize=6, linestyle = '')\n",
    "         \n",
    "        \n",
    "        for i in range(self.agent):\n",
    "            ax[plot_number].plot(self.current_state[i][0], self.current_state[i][1], color='blue', marker='x', markersize=9, linestyle = '', label = \"UAV\")\n",
    "            UAV_coverage_area = plt.Circle((self.current_state[i][0] , self.current_state[i][1] ), self.C_max_t, color = self.UAV_coverage_circle_color)\n",
    "            ax[plot_number].add_artist(UAV_coverage_area)\n",
    "      \n",
    "\n",
    "        if 'learnt_policy_visualization' in kwargs:\n",
    "            position_action_list = kwargs['position_action_list']\n",
    "            for position_action in position_action_list:\n",
    "                position = position_action[0]\n",
    "                action = position_action[1]\n",
    "                action = np.clip(action, self.action_space_lb, self.action_space_ub)\n",
    "                action = (((self.action_space_coversion_ub - self.action_space_coversion_lb) * (action + 1)) / 2) + self.action_space_coversion_lb\n",
    "                print(\"scaled action: \", action)\n",
    "                # print(\"action after scaling: \", action)\n",
    "                new_state = self.move(action, get_new_state=True, provided_center=position)\n",
    "                print(\"previous state: \", position, \", new state: \", new_state)\n",
    "                # print(\"new_state: \", new_state)\n",
    "                ax[plot_number].plot(position[0], position[1], color='blue', marker='o', markersize=6, linestyle = '')\n",
    "                ax[plot_number].plot(new_state[0], new_state[1], color='orange', marker='o', markersize=6, linestyle = '')\n",
    "                ax[plot_number].arrow(position[0], position[1], new_state[0] - position[0], new_state[1] - position[1], head_width=0.2, head_length=0.1)\n",
    "\n",
    "\n",
    "        ax[plot_number].set_aspect(1)\n",
    "        # naming the x axis\n",
    "        ax[plot_number].set_xlabel('X pos (m)')\n",
    "        # naming the y axis\n",
    "        ax[plot_number].set_ylabel('Y pos (m)')\n",
    "        ax[plot_number].set_title('reward: ' + str(reward))\n",
    "        # giving a title to my graph\n",
    "        # plt.title('Visually Appealing!')\n",
    "\n",
    "        # show a legend on the plot\n",
    "        ax[plot_number].legend()\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Multi_Agent_UAV()\n",
    "state = env.reset()\n",
    "UAV_positions_list = []\n",
    "ab=[]\n",
    "for i in range(env.agent):\n",
    "    k = []\n",
    "    k.append(env.current_state[i][0])\n",
    "    k.append(env.current_state[i][1])\n",
    "    ab.append(k)\n",
    "UAV_positions_list.append(ab)\n",
    "a = [env.action_space.sample() for i in range(env.agent)]\n",
    "# print(a)\n",
    "next_state, reward, done, _ = env.step(a)\n",
    "print(next_state, reward)\n",
    "ab=[]\n",
    "for i in range(env.agent):\n",
    "    k = []\n",
    "    k.append(env.current_state[i][0])\n",
    "    k.append(env.current_state[i][1])\n",
    "    ab.append(k)\n",
    "UAV_positions_list.append(ab)\n",
    "b = [env.action_space.sample() for i in range(env.agent)]\n",
    "# print(b)\n",
    "next_state, reward, done, _ = env.step(b)\n",
    "# print(next_state, reward)\n",
    "ab=[]\n",
    "for i in range(env.agent):\n",
    "    k = []\n",
    "    k.append(env.current_state[i][0])\n",
    "    k.append(env.current_state[i][1])\n",
    "    ab.append(k)\n",
    "UAV_positions_list.append(ab)\n",
    "UAV_positions_list = np.array(UAV_positions_list)\n",
    "print(\"List\",UAV_positions_list)\n",
    "env.render_UAV_movement_through_episode(UAV_positions_list = UAV_positions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220da032",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 10000\n",
    "import copy\n",
    "       \n",
    "action_std = 0.6                    \n",
    "action_std_decay_rate = 0.05       \n",
    "min_action_std = 0.1                \n",
    "\n",
    "  \n",
    "K_epochs = 30        \n",
    "\n",
    "eps_clip = 0.2         \n",
    "gamma = 0.99           \n",
    "max_steps=50\n",
    "update=max_steps*4\n",
    "overallsteps=200000\n",
    "lr_actor = 0.0003      \n",
    "lr_critic = 0.001      \n",
    "string='Multi_Agent_UAV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db2cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import math\n",
    "\n",
    "# Uncommenting this for actual implementation\n",
    "# device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
    "device = T.device(\"cpu\")  # Placeholder for the code review\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "        \n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, action_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DoubleDQNAgent:\n",
    "    def __init__(self, envstr, learning_rate=0.0003, gamma=0.99, epsilon_start=1.0, \n",
    "                 epsilon_end=0.01, epsilon_decay=0.995, buffer_size=100000, \n",
    "                 batch_size=64, target_update=100):\n",
    "        env = Multi_Agent_UAV()\n",
    "        \n",
    "        # Define the number of continuous action dimensions\n",
    "        self.action_dims = 2  # Two dimensions: speed and angle\n",
    "        # Define the number of discrete divisions per dimension\n",
    "        self.divisions_per_dim = 5  # 5 discrete values per dimension\n",
    "        \n",
    "        # Total number of discrete actions (combinations)\n",
    "        self.action_dim = self.divisions_per_dim ** self.action_dims\n",
    "        self.state_dim = env.observation_space[0].shape[0]\n",
    "        \n",
    "        # Q Networks\n",
    "        self.q_network = QNetwork(self.state_dim, self.action_dim).to(device)\n",
    "        self.target_network = QNetwork(self.state_dim, self.action_dim).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "        \n",
    "        # Create action mapping (discrete index to continuous vector)\n",
    "        self.action_space = []\n",
    "        action_values = np.linspace(-1, 1, self.divisions_per_dim)\n",
    "        \n",
    "        # Create all combinations of actions\n",
    "        for i in range(self.divisions_per_dim):\n",
    "            for j in range(self.divisions_per_dim):\n",
    "                self.action_space.append([action_values[i], action_values[j]])\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # Counter for target network updates\n",
    "        self.update_counter = 0\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Random action\n",
    "            action_index = np.random.randint(0, self.action_dim)\n",
    "            # Map to continuous action vector\n",
    "            return self.action_space[action_index]\n",
    "        \n",
    "        with T.no_grad():\n",
    "            state = T.FloatTensor(np.array(state).reshape(1, -1)).to(device)\n",
    "            q_values = self.q_network(state)\n",
    "            action_index = T.argmax(q_values, dim=1).item()\n",
    "            # Map to continuous action vector\n",
    "            return self.action_space[action_index]\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample a batch from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = T.FloatTensor(np.array(states)).to(device)\n",
    "        actions = T.LongTensor(np.array(actions)).to(device)\n",
    "        rewards = T.FloatTensor(np.array(rewards)).to(device)\n",
    "        next_states = T.FloatTensor(np.array(next_states)).to(device)\n",
    "        dones = T.FloatTensor(np.array(dones)).to(device)\n",
    "        \n",
    "        # Get Q values for current states and actions\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Double DQN: Use online network to select actions and target network to evaluate them\n",
    "        with T.no_grad():\n",
    "            # Get actions that would be taken by online network\n",
    "            online_next_q_values = self.q_network(next_states)\n",
    "            online_next_actions = T.argmax(online_next_q_values, dim=1, keepdim=True)\n",
    "            \n",
    "            # Evaluate those actions using the target network\n",
    "            next_q_values = self.target_network(next_states).gather(1, online_next_actions)\n",
    "            target_q_values = rewards.unsqueeze(1) + (1 - dones.unsqueeze(1)) * self.gamma * next_q_values\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(q_values, target_q_values)\n",
    "        \n",
    "        # Update Q network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping similar to original code\n",
    "        for p in self.q_network.modules():\n",
    "            T.nn.utils.clip_grad_norm_(p.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        self.update_counter += 1\n",
    "        if self.update_counter % self.target_update == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def save_experience(self, state, action, reward, next_state, done):\n",
    "        # Find the closest discrete action in our action space\n",
    "        action_idx = self.get_closest_action_index(action)\n",
    "        self.buffer.push(state, action_idx, reward, next_state, done)\n",
    "    \n",
    "    def get_closest_action_index(self, action):\n",
    "        # Find the index of the closest action in the discrete action space\n",
    "        distances = [np.sum((np.array(a) - np.array(action))**2) for a in self.action_space]\n",
    "        return np.argmin(distances)\n",
    "\n",
    "# Training loop\n",
    "def train_double_dqn(env, agents, max_episodes=1000, max_steps=500):\n",
    "    step = 0\n",
    "    rp = 0\n",
    "    scores = []\n",
    "    \n",
    "    while rp <= max_episodes:\n",
    "        interscore = 0\n",
    "        \n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        for j in range(max_steps):\n",
    "            step += 1\n",
    "            \n",
    "            # Get actions from agents\n",
    "            actions = []\n",
    "            for i in range(env.agent):\n",
    "                action = agents[i].get_action(state[i])\n",
    "                actions.append(action)\n",
    "            \n",
    "            # Take step in environment\n",
    "            next_state, reward, done, _ = env.step(actions)\n",
    "            interscore += reward\n",
    "            \n",
    "            # Store experience in each agent's buffer\n",
    "            for i in range(env.agent):\n",
    "                agents[i].save_experience(state[i], actions[i], reward, next_state[i], done)\n",
    "                \n",
    "                # Update Double DQN\n",
    "                if step % 4 == 0:  # Update frequency\n",
    "                    agents[i].update()\n",
    "                \n",
    "                # Decay epsilon\n",
    "                if step % 1000 == 0:\n",
    "                    agents[i].decay_epsilon()\n",
    "            \n",
    "            state = next_state\n",
    "            scores.append(interscore)\n",
    "            \n",
    "            if done:\n",
    "                rp += 1\n",
    "                break\n",
    "        \n",
    "        if rp % 40 == 0:\n",
    "            print(np.mean(scores[-100:]), step, \" episodes : \", rp)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Initialize environment and agents\n",
    "env = Multi_Agent_UAV()\n",
    "agents = [DoubleDQNAgent(\n",
    "    envstr=\"Multi_Agent_UAV\",\n",
    "    learning_rate=0.0001,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.3,\n",
    "    epsilon_decay=0.999,\n",
    "    buffer_size=1000,\n",
    "    batch_size=32,\n",
    "    target_update=1000\n",
    ") for _ in range(env.agent)]\n",
    "\n",
    "# Train agents\n",
    "scores = train_double_dqn(env, agents, max_episodes=max_episodes, max_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_trained_agent(env, agents):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    UAV_positions_list = []\n",
    "    \n",
    "    while not done:\n",
    "        # Get actions from all agents using the trained DQN\n",
    "        actions = []\n",
    "        for i in range(env.agent):\n",
    "            # Get action without exploration (epsilon=0)\n",
    "            with T.no_grad():\n",
    "                state_tensor = T.FloatTensor(np.array(state[i]).reshape(1, -1)).to(device)\n",
    "                q_values = agents[i].q_network(state_tensor)\n",
    "                action_index = T.argmax(q_values, dim=1).item()\n",
    "                action = agents[i].action_space[action_index]\n",
    "            actions.append(action)\n",
    "        \n",
    "        # Take step in environment\n",
    "        state, reward, done, _ = env.step(actions)\n",
    "        score += reward\n",
    "        \n",
    "        # Record UAV positions\n",
    "        ab = []\n",
    "        for i in range(env.agent):\n",
    "            k = []\n",
    "            k.append(env.current_state[i][0])\n",
    "            k.append(env.current_state[i][1])\n",
    "            ab.append(k)\n",
    "        UAV_positions_list.append(ab)\n",
    "        \n",
    "        print('score %.1f' % score, 'avg_score %.1f' % score)\n",
    "    \n",
    "    # Convert to numpy array for rendering\n",
    "    UAV_positions_list = np.array(UAV_positions_list)\n",
    "    \n",
    "    # Render the UAV movement\n",
    "    env.render_UAV_movement_through_episode(UAV_positions_list=UAV_positions_list)\n",
    "    \n",
    "    return score\n",
    "\n",
    "# After training is complete, visualize the trained agents\n",
    "final_score = visualize_trained_agent(env, agents)\n",
    "print(f\"Final visualization score: {final_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f12f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data={'scores':scores})\n",
    "df2=df.copy()\n",
    "df2['max']=df2.scores.rolling(100).mean()\n",
    "df2.drop(columns=['scores'],inplace=True)\n",
    "\n",
    "df['mean']=df.scores.rolling(100).mean()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c78cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(figsize=(18,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e24a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_mean(a,shift):\n",
    "    cur_mean=np.mean(a[:shift])\n",
    "    result=[cur_mean]\n",
    "    \n",
    "    \n",
    "    for i in range(len(a)-shift):\n",
    "        cur_mean+=(a[i+shift]-a[i])/shift\n",
    "        result.append(cur_mean)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d5bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "utility=calculate_rolling_mean(list(df2.loc[:,'max'].dropna()),500)\n",
    "plt.plot(range(len(utility)),utility)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"avg. utility\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3986452",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_plot=[np.mean(i) for i in env.energy_array]\n",
    "shift=500\n",
    "energy_plot=calculate_rolling_mean(energy_plot,shift)\n",
    "plt.plot(range(len(energy_plot)),energy_plot)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"avg. energy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3acc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi_plot=[np.mean(i) for i in env.AOI_array]\n",
    "shift=500\n",
    "aoi_plot=calculate_rolling_mean(aoi_plot,shift)\n",
    "plt.plot(range(len(aoi_plot)),aoi_plot)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"avg. AOI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b23a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "result_count=\"doubleDQN\"\n",
    "save_to_file=True\n",
    "if save_to_file:\n",
    "    with open(f\"./results/result_{result_count}_utility.json\",\"w\") as f:\n",
    "        f.write(json.dumps(utility))\n",
    "    with open(f\"./results/result_{result_count}_energy.json\",\"w\") as f:\n",
    "        f.write(json.dumps(env.energy_array))\n",
    "    with open(f\"./results/result_{result_count}_AOI.json\",\"w\") as f:\n",
    "        f.write(json.dumps(env.AOI_array))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
